"""
EPM Note Engine - Admin Components

Knowledge base management and system administration UI.
"""

import json
import os
import re
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path

import streamlit as st

from src.repositories.rag_service import RAGService


# Field length limits based on Article model
FIELD_LIMITS = {
    "week_id": 50,
    "title": 255,
    "target_persona": 255,
    "seo_keywords": 255,
    "hook_statement": None,  # Text field, no limit
    "content_outline": None,  # Text field, no limit
}


@dataclass
class ValidationError:
    """Validation error with field name and message."""
    field: str
    message: str


@dataclass
class ArticleData:
    """Parsed article data for import."""
    week_id: str
    title: str
    target_persona: str | None = None
    hook_statement: str | None = None
    content_outline: str | None = None
    seo_keywords: str | None = None


def render_help_popover(label: str, body: str | list[str]) -> None:
    """Render a small help popover."""
    with st.popover(label):
        if isinstance(body, list):
            for line in body:
                st.markdown(f"- {line}")
        else:
            st.markdown(body)


def validate_article_data(
    data: ArticleData,
    existing_week_ids: set[str] | None = None,
) -> list[ValidationError]:
    """
    Validate article data against model constraints.

    Args:
        data: Article data to validate.
        existing_week_ids: Set of existing week IDs to check for duplicates.

    Returns:
        List of validation errors (empty if valid).
    """
    errors = []

    # Required fields
    if not data.week_id or not data.week_id.strip():
        errors.append(ValidationError("week_id", "Week ID ã¯å¿…é ˆã§ã™"))
    elif len(data.week_id) > FIELD_LIMITS["week_id"]:
        errors.append(ValidationError(
            "week_id",
            f"Week ID ã¯{FIELD_LIMITS['week_id']}æ–‡å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„ï¼ˆç¾åœ¨: {len(data.week_id)}æ–‡å­—ï¼‰"
        ))
    elif existing_week_ids and data.week_id in existing_week_ids:
        errors.append(ValidationError("week_id", f"Week ID '{data.week_id}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™"))

    if not data.title or not data.title.strip():
        errors.append(ValidationError("title", "ã‚¿ã‚¤ãƒˆãƒ«ã¯å¿…é ˆã§ã™"))
    elif len(data.title) > FIELD_LIMITS["title"]:
        errors.append(ValidationError(
            "title",
            f"ã‚¿ã‚¤ãƒˆãƒ«ã¯{FIELD_LIMITS['title']}æ–‡å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„ï¼ˆç¾åœ¨: {len(data.title)}æ–‡å­—ï¼‰"
        ))

    # Optional fields with length limits
    if data.target_persona and len(data.target_persona) > FIELD_LIMITS["target_persona"]:
        errors.append(ValidationError(
            "target_persona",
            f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ«ã‚½ãƒŠã¯{FIELD_LIMITS['target_persona']}æ–‡å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„"
        ))

    if data.seo_keywords and len(data.seo_keywords) > FIELD_LIMITS["seo_keywords"]:
        errors.append(ValidationError(
            "seo_keywords",
            f"SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯{FIELD_LIMITS['seo_keywords']}æ–‡å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„"
        ))

    return errors


def parse_markdown_articles(content: str) -> list[ArticleData]:
    """
    Parse markdown format article list.

    Expected format:
    ## Week1-1: ã‚¿ã‚¤ãƒˆãƒ«
    - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: CFO, çµŒå–¶ä¼ç”»
    - ãƒ•ãƒƒã‚¯: ã€Œä¼šè­°ã®ä¸€è¨€ã€
    - è¦‹å‡ºã—: å•é¡Œ â†’ åŸå›  â†’ è§£æ±ºç­–
    - SEO: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2

    Args:
        content: Markdown content to parse.

    Returns:
        List of parsed ArticleData.
    """
    articles = []

    # Pattern for article header: ## WeekX-Y: Title or ### N. Title
    # Also supports: ## WeekX-Y ã‚¿ã‚¤ãƒˆãƒ« (without colon)
    header_pattern = re.compile(
        r"^##\s+(?:(\d+)\.\s+)?(?:(Week[\w-]+)[:\s]+)?(.+?)$",
        re.MULTILINE | re.IGNORECASE,
    )

    matches = list(header_pattern.finditer(content))

    for i, match in enumerate(matches):
        number = match.group(1)
        week_id = match.group(2)
        title = match.group(3).strip()

        # If no week_id but has number, generate week_id
        if not week_id and number:
            week_num = (int(number) + 1) // 2
            day_suffix = 1 if int(number) % 2 == 1 else 2
            week_id = f"Week{week_num}-{day_suffix}"
        elif not week_id:
            # Try to extract from title or use placeholder
            week_id = f"NEW-{i+1}"

        # Get section content
        start_pos = match.end()
        end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        section = content[start_pos:end_pos]

        # Parse fields from section
        target = extract_markdown_field(section, ["ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ", "ãƒšãƒ«ã‚½ãƒŠ", "target", "persona"])
        hook = extract_markdown_field(section, ["ãƒ•ãƒƒã‚¯", "ä¼šè­°", "hook", "å°å…¥"])
        outline = extract_markdown_field(section, ["è¦‹å‡ºã—", "æ§‹æˆ", "ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³", "outline"])
        seo = extract_markdown_field(section, ["SEO", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "keyword"])

        articles.append(ArticleData(
            week_id=week_id,
            title=title,
            target_persona=target,
            hook_statement=hook,
            content_outline=outline,
            seo_keywords=seo,
        ))

    return articles


def extract_markdown_field(section: str, field_names: list[str]) -> str | None:
    """Extract a field value from markdown section."""
    for name in field_names:
        # Pattern: - field_name: value or - field_nameï¼švalue
        pattern = re.compile(
            rf"^[-*]\s*{re.escape(name)}[ï¼š:]\s*(.+?)$",
            re.MULTILINE | re.IGNORECASE,
        )
        match = pattern.search(section)
        if match:
            value = match.group(1).strip()
            # Remove markdown formatting
            value = re.sub(r"\*\*([^*]+)\*\*", r"\1", value)
            value = re.sub(r"\*([^*]+)\*", r"\1", value)
            return value.strip() if value.strip() else None
    return None


def parse_json_articles(content: str) -> list[ArticleData]:
    """
    Parse JSON format article list.

    Expected format:
    [
      {
        "week_id": "Week1-1",
        "title": "ã‚¿ã‚¤ãƒˆãƒ«",
        "target_persona": "CFO",
        "hook_statement": "ãƒ•ãƒƒã‚¯",
        "content_outline": "è¦‹å‡ºã—æ¡ˆ",
        "seo_keywords": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"
      }
    ]

    Args:
        content: JSON content to parse.

    Returns:
        List of parsed ArticleData.
    """
    try:
        data = json.loads(content)
        if not isinstance(data, list):
            data = [data]

        articles = []
        for item in data:
            if isinstance(item, dict):
                articles.append(ArticleData(
                    week_id=str(item.get("week_id", "")),
                    title=str(item.get("title", "")),
                    target_persona=item.get("target_persona"),
                    hook_statement=item.get("hook_statement"),
                    content_outline=item.get("content_outline"),
                    seo_keywords=item.get("seo_keywords"),
                ))
        return articles
    except json.JSONDecodeError as e:
        raise ValueError(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")


def parse_tsv_articles(content: str) -> list[ArticleData]:
    """
    Parse TSV (Tab-Separated Values) format article list.
    This is the format when copying from Excel.

    Expected format (first row is header):
    week_id	title	target_persona	hook_statement	content_outline	seo_keywords
    Week1-1	ã‚¿ã‚¤ãƒˆãƒ«1	CFO	ãƒ•ãƒƒã‚¯1	è¦‹å‡ºã—1	ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1
    Week1-2	ã‚¿ã‚¤ãƒˆãƒ«2	çµŒå–¶ä¼ç”»	ãƒ•ãƒƒã‚¯2	è¦‹å‡ºã—2	ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2

    Args:
        content: TSV content to parse (copied from Excel).

    Returns:
        List of parsed ArticleData.
    """
    lines = content.strip().split("\n")
    if len(lines) < 2:
        raise ValueError("ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¨ãƒ‡ãƒ¼ã‚¿è¡ŒãŒå¿…è¦ã§ã™ï¼ˆæœ€ä½2è¡Œï¼‰")

    # Parse header row to determine column mapping
    header = lines[0].split("\t")
    header_lower = [h.strip().lower() for h in header]

    # Column name mapping (flexible matching)
    column_mapping = {
        "week_id": ["week_id", "weekid", "week id", "é€±id", "é€±"],
        "title": ["title", "ã‚¿ã‚¤ãƒˆãƒ«", "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«", "é¡Œå"],
        "target_persona": ["target_persona", "target", "persona", "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ", "ãƒšãƒ«ã‚½ãƒŠ", "æƒ³å®šèª­è€…"],
        "hook_statement": ["hook_statement", "hook", "ãƒ•ãƒƒã‚¯", "ä¼šè­°ã®ä¸€è¨€", "å°å…¥"],
        "content_outline": ["content_outline", "outline", "è¦‹å‡ºã—", "æ§‹æˆ", "ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³"],
        "seo_keywords": ["seo_keywords", "seo", "keywords", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "seoã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"],
    }

    # Find column indices
    col_indices = {}
    for field, aliases in column_mapping.items():
        for i, h in enumerate(header_lower):
            if h in aliases or any(alias in h for alias in aliases):
                col_indices[field] = i
                break

    # Require at least week_id and title
    if "week_id" not in col_indices:
        raise ValueError("Week ID åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã« 'week_id' ã¾ãŸã¯ 'é€±ID' ã‚’å«ã‚ã¦ãã ã•ã„ã€‚")
    if "title" not in col_indices:
        raise ValueError("ã‚¿ã‚¤ãƒˆãƒ«åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã« 'title' ã¾ãŸã¯ 'ã‚¿ã‚¤ãƒˆãƒ«' ã‚’å«ã‚ã¦ãã ã•ã„ã€‚")

    articles = []
    for i, line in enumerate(lines[1:], start=2):
        if not line.strip():
            continue

        cols = line.split("\t")

        def get_col(field: str) -> str | None:
            idx = col_indices.get(field)
            if idx is not None and idx < len(cols):
                val = cols[idx].strip()
                return val if val else None
            return None

        week_id = get_col("week_id") or ""
        title = get_col("title") or ""

        if not week_id and not title:
            continue  # Skip empty rows

        articles.append(ArticleData(
            week_id=week_id,
            title=title,
            target_persona=get_col("target_persona"),
            hook_statement=get_col("hook_statement"),
            content_outline=get_col("content_outline"),
            seo_keywords=get_col("seo_keywords"),
        ))

    return articles


def render_admin_panel() -> None:
    """Render the admin panel for knowledge base management."""
    st.header("ç®¡ç†ãƒ‘ãƒãƒ«")

    tab1, tab2, tab3, tab4 = st.tabs(["è¨˜äº‹ç®¡ç†", "çŸ¥è­˜ãƒ™ãƒ¼ã‚¹", "æ¤œç´¢ãƒ†ã‚¹ãƒˆ", "ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±"])

    with tab1:
        render_article_management_tab()

    with tab2:
        render_knowledge_base_tab()

    with tab3:
        render_search_test_tab()

    with tab4:
        render_system_info_tab()


def render_article_management_tab() -> None:
    """Render article management tab."""
    st.subheader("è¨˜äº‹ç®¡ç†")

    # Three sections: Add, Bulk Import, and List
    add_section, import_section, list_section = st.tabs([
        "â• æ–°è¦è¿½åŠ ",
        "ğŸ“¥ ä¸€æ‹¬å–è¾¼",
        "ğŸ“‹ ä¸€è¦§ãƒ»å‰Šé™¤",
    ])

    with add_section:
        render_article_add_form()

    with import_section:
        render_article_import_form()

    with list_section:
        render_article_list()


def render_article_add_form() -> None:
    """Render form to add a new article."""
    from src.database.connection import get_session
    from src.database.models import Article, ArticleStatus
    from src.repositories.article_repository import ArticleRepository

    st.markdown("### æ–°è¦è¨˜äº‹ã‚’è¿½åŠ ")

    # Show field limits
    with st.expander("ğŸ“ å…¥åŠ›åˆ¶é™", expanded=False):
        st.caption(f"- Week ID: æœ€å¤§{FIELD_LIMITS['week_id']}æ–‡å­—")
        st.caption(f"- ã‚¿ã‚¤ãƒˆãƒ«: æœ€å¤§{FIELD_LIMITS['title']}æ–‡å­—")
        st.caption(f"- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ«ã‚½ãƒŠ: æœ€å¤§{FIELD_LIMITS['target_persona']}æ–‡å­—")
        st.caption(f"- SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: æœ€å¤§{FIELD_LIMITS['seo_keywords']}æ–‡å­—")

    with st.form(key="add_article_form"):
        # Required fields
        col1, col2 = st.columns(2)
        with col1:
            week_id = st.text_input(
                "Week ID *",
                placeholder="ä¾‹: Week1-1",
                help=f"é€±ç•ªå·ã¨æ›œæ—¥ï¼ˆ1=ç«æ›œ, 2=é‡‘æ›œï¼‰- æœ€å¤§{FIELD_LIMITS['week_id']}æ–‡å­—",
                max_chars=FIELD_LIMITS["week_id"],
            )
        with col2:
            title = st.text_input(
                "ã‚¿ã‚¤ãƒˆãƒ« *",
                placeholder="ä¾‹: äºˆå®Ÿç®¡ç†ã€Œæ•°å­—ãŒåˆã‚ãªã„ã€å•é¡Œã®æ­£ä½“",
                max_chars=FIELD_LIMITS["title"],
            )

        # Optional fields
        target_persona = st.text_input(
            "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ«ã‚½ãƒŠ",
            placeholder="ä¾‹: CFO, çµŒå–¶ä¼ç”»",
            help="ã“ã®è¨˜äº‹ã®æƒ³å®šèª­è€…",
            max_chars=FIELD_LIMITS["target_persona"],
        )

        hook_statement = st.text_area(
            "ãƒ•ãƒƒã‚¯ï¼ˆä¼šè­°ã®ä¸€è¨€ï¼‰",
            placeholder="ä¾‹: ã€Œãã®æ•°å­—ã€è³‡æ–™ã”ã¨ã«é•ã†ã‘ã©ã€ã©ã‚ŒãŒæ­£ã—ã„ã®ï¼Ÿã€",
            help="èª­è€…ã®æ³¨æ„ã‚’å¼•ãä¸€è¨€",
            height=80,
        )

        content_outline = st.text_area(
            "è¦‹å‡ºã—æ¡ˆ",
            placeholder="ä¾‹: å•é¡Œã®æœ¬è³ª â†’ 3ã¤ã®åŸå›  â†’ è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ â†’ å…·ä½“çš„ãªã‚¹ãƒ†ãƒƒãƒ—",
            help="è¨˜äº‹ã®æ§‹æˆæ¡ˆï¼ˆâ†’ã§åŒºåˆ‡ã‚‹ï¼‰",
            height=100,
        )

        seo_keywords = st.text_input(
            "SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            placeholder="ä¾‹: äºˆå®Ÿç®¡ç†, æ•°å­—ãŒåˆã‚ãªã„, çµŒå–¶ç®¡ç†",
            help="ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å…¥åŠ›å¯èƒ½ï¼ˆå¾Œã‹ã‚‰ãƒªã‚µãƒ¼ãƒæ™‚ã«è¨­å®šã‚‚å¯ï¼‰",
            max_chars=FIELD_LIMITS["seo_keywords"],
        )

        submitted = st.form_submit_button(
            "ğŸ“ è¨˜äº‹ã‚’è¿½åŠ ",
            type="primary",
            use_container_width=True,
        )

        if submitted:
            try:
                with get_session() as session:
                    repo = ArticleRepository(session)

                    # Get existing week IDs for duplicate check
                    existing_week_ids = {a.week_id for a in repo.get_all()}

                    # Validate using shared validation function
                    article_data = ArticleData(
                        week_id=week_id,
                        title=title,
                        target_persona=target_persona or None,
                        hook_statement=hook_statement or None,
                        content_outline=content_outline or None,
                        seo_keywords=seo_keywords or None,
                    )
                    errors = validate_article_data(article_data, existing_week_ids)

                    if errors:
                        for error in errors:
                            st.error(f"âŒ {error.field}: {error.message}")
                    else:
                        # Create new article
                        article = Article(
                            week_id=week_id,
                            title=title,
                            target_persona=target_persona or None,
                            hook_statement=hook_statement or None,
                            content_outline=content_outline or None,
                            seo_keywords=seo_keywords or None,
                            status=ArticleStatus.PLANNING,
                        )
                        repo.create(article)
                        session.commit()
                        st.success(f"âœ… è¨˜äº‹ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {title}")
                        st.rerun()
            except Exception as e:
                st.error(f"è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def render_article_import_form() -> None:
    """Render bulk import form for articles."""
    from src.database.connection import get_session
    from src.database.models import Article, ArticleStatus
    from src.repositories.article_repository import ArticleRepository

    st.markdown("### ä¸€æ‹¬å–è¾¼")

    # Format selection
    format_type = st.radio(
        "å…¥åŠ›å½¢å¼",
        ["Excel (ã‚³ãƒ”ãƒš)", "Markdown", "JSON"],
        horizontal=True,
        help="Excelã‹ã‚‰ã‚³ãƒ”ãƒ¼ã€ç”ŸæˆAIã®å‡ºåŠ›å½¢å¼ã«åˆã‚ã›ã¦é¸æŠ",
    )

    # Show format examples
    with st.expander("ğŸ“ å…¥åŠ›å½¢å¼ã®ä¾‹", expanded=False):
        if format_type == "Excel (ã‚³ãƒ”ãƒš)":
            st.markdown("""
**Excelå½¢å¼** - Excelã§è¨˜äº‹ãƒªã‚¹ãƒˆã‚’ç®¡ç†ã—ã¦ã„ã‚‹å ´åˆã«ä¾¿åˆ©

1. ä»¥ä¸‹ã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å«ã‚€Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼š

| week_id | title | target_persona | hook_statement | content_outline | seo_keywords |
|---------|-------|----------------|----------------|-----------------|--------------|
| Week1-1 | ã‚¿ã‚¤ãƒˆãƒ«1 | CFO | ãƒ•ãƒƒã‚¯1 | è¦‹å‡ºã—1 | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1 |
| Week1-2 | ã‚¿ã‚¤ãƒˆãƒ«2 | çµŒå–¶ä¼ç”» | ãƒ•ãƒƒã‚¯2 | è¦‹å‡ºã—2 | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2 |

2. ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€ï¼‰ã‚’é¸æŠã—ã¦ã‚³ãƒ”ãƒ¼ (Ctrl+C)
3. ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã«è²¼ã‚Šä»˜ã‘ (Ctrl+V)

**ãƒ˜ãƒƒãƒ€ãƒ¼åã¯æŸ”è»Ÿã«å¯¾å¿œ:**
- `week_id`, `é€±ID`, `Week ID` â†’ Week ID
- `title`, `ã‚¿ã‚¤ãƒˆãƒ«` â†’ ã‚¿ã‚¤ãƒˆãƒ«
- `target_persona`, `ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ`, `ãƒšãƒ«ã‚½ãƒŠ` â†’ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
- `hook_statement`, `ãƒ•ãƒƒã‚¯`, `ä¼šè­°ã®ä¸€è¨€` â†’ ãƒ•ãƒƒã‚¯
- `content_outline`, `è¦‹å‡ºã—`, `æ§‹æˆ` â†’ è¦‹å‡ºã—
- `seo_keywords`, `SEO`, `ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰` â†’ SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            """)
        elif format_type == "Markdown":
            st.markdown("""
**Markdownå½¢å¼** - ç”ŸæˆAIã«è¨˜äº‹å€™è£œã‚’ä½œã£ã¦ã‚‚ã‚‰ã†å ´åˆã«ä¾¿åˆ©

```markdown
## Week1-1: äºˆå®Ÿç®¡ç†ã€Œæ•°å­—ãŒåˆã‚ãªã„ã€å•é¡Œã®æ­£ä½“
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: CFO, çµŒå–¶ä¼ç”»
- ãƒ•ãƒƒã‚¯: ã€Œãã®æ•°å­—ã€ã©ã‚ŒãŒæ­£ã—ã„ã®ï¼Ÿã€
- è¦‹å‡ºã—: å•é¡Œã®æœ¬è³ª â†’ 3ã¤ã®åŸå›  â†’ è§£æ±ºç­–
- SEO: äºˆå®Ÿç®¡ç†, æ•°å­—, çµŒå–¶ç®¡ç†

## Week1-2: KPIè¨­è¨ˆã®è½ã¨ã—ç©´
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: äº‹æ¥­éƒ¨é•·, FP&A
- ãƒ•ãƒƒã‚¯: ã€ŒKPIã‚’è¨­å®šã—ãŸã®ã«æˆæœãŒå‡ºãªã„ã€
- è¦‹å‡ºã—: ã‚ˆãã‚ã‚‹å¤±æ•— â†’ æ­£ã—ã„è¨­è¨ˆæ‰‹é † â†’ é‹ç”¨ã®ã‚³ãƒ„
- SEO: KPI, è¨­è¨ˆ, çµŒå–¶æŒ‡æ¨™
```

ã¾ãŸã¯ç•ªå·å½¢å¼:
```markdown
## 1. äºˆå®Ÿç®¡ç†ã€Œæ•°å­—ãŒåˆã‚ãªã„ã€å•é¡Œã®æ­£ä½“
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: CFO

## 2. KPIè¨­è¨ˆã®è½ã¨ã—ç©´
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: äº‹æ¥­éƒ¨é•·
```
            """)
        else:
            st.markdown("""
**JSONå½¢å¼** - ãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ã«ç”Ÿæˆã™ã‚‹å ´åˆã«ä¾¿åˆ©

```json
[
  {
    "week_id": "Week1-1",
    "title": "äºˆå®Ÿç®¡ç†ã€Œæ•°å­—ãŒåˆã‚ãªã„ã€å•é¡Œã®æ­£ä½“",
    "target_persona": "CFO, çµŒå–¶ä¼ç”»",
    "hook_statement": "ã€Œãã®æ•°å­—ã€ã©ã‚ŒãŒæ­£ã—ã„ã®ï¼Ÿã€",
    "content_outline": "å•é¡Œã®æœ¬è³ª â†’ 3ã¤ã®åŸå›  â†’ è§£æ±ºç­–",
    "seo_keywords": "äºˆå®Ÿç®¡ç†, æ•°å­—, çµŒå–¶ç®¡ç†"
  },
  {
    "week_id": "Week1-2",
    "title": "KPIè¨­è¨ˆã®è½ã¨ã—ç©´",
    "target_persona": "äº‹æ¥­éƒ¨é•·, FP&A"
  }
]
```
            """)

    # Input area
    import_content = st.text_area(
        "ãƒ‡ãƒ¼ã‚¿ã‚’è²¼ã‚Šä»˜ã‘",
        height=300,
        placeholder="ä¸Šè¨˜ã®å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„...",
        key="import_content",
    )

    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ” ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", use_container_width=True):
            if not import_content.strip():
                st.warning("ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                try:
                    # Parse based on format
                    if format_type == "Excel (ã‚³ãƒ”ãƒš)":
                        articles = parse_tsv_articles(import_content)
                    elif format_type == "Markdown":
                        articles = parse_markdown_articles(import_content)
                    else:
                        articles = parse_json_articles(import_content)

                    if not articles:
                        st.warning("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    else:
                        st.session_state["preview_articles"] = articles
                        st.success(f"âœ… {len(articles)} ä»¶ã®è¨˜äº‹ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                except Exception as e:
                    st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {e}")

    # Show preview if available
    if "preview_articles" in st.session_state and st.session_state["preview_articles"]:
        articles = st.session_state["preview_articles"]

        st.divider()
        st.markdown("#### ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")

        try:
            with get_session() as session:
                repo = ArticleRepository(session)
                existing_week_ids = {a.week_id for a in repo.get_all()}

                valid_articles = []
                has_errors = False

                for i, article in enumerate(articles):
                    errors = validate_article_data(article, existing_week_ids)

                    with st.expander(
                        f"{'âŒ' if errors else 'âœ…'} {article.week_id}: {article.title[:40]}...",
                        expanded=bool(errors),
                    ):
                        if errors:
                            has_errors = True
                            for error in errors:
                                st.error(f"{error.field}: {error.message}")
                        else:
                            valid_articles.append(article)

                        st.markdown(f"**ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ:** {article.target_persona or '(æœªè¨­å®š)'}")
                        st.markdown(f"**ãƒ•ãƒƒã‚¯:** {article.hook_statement or '(æœªè¨­å®š)'}")
                        st.markdown(f"**è¦‹å‡ºã—:** {article.content_outline or '(æœªè¨­å®š)'}")
                        st.markdown(f"**SEO:** {article.seo_keywords or '(æœªè¨­å®š)'}")

                st.divider()

                # Import button
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("æœ‰åŠ¹", f"{len(valid_articles)} ä»¶")
                with col2:
                    st.metric("ã‚¨ãƒ©ãƒ¼", f"{len(articles) - len(valid_articles)} ä»¶")

                if valid_articles:
                    if st.button(
                        f"ğŸ“¥ {len(valid_articles)} ä»¶ã‚’å–è¾¼",
                        type="primary",
                        use_container_width=True,
                    ):
                        imported = 0
                        for article_data in valid_articles:
                            article = Article(
                                week_id=article_data.week_id,
                                title=article_data.title,
                                target_persona=article_data.target_persona,
                                hook_statement=article_data.hook_statement,
                                content_outline=article_data.content_outline,
                                seo_keywords=article_data.seo_keywords,
                                status=ArticleStatus.PLANNING,
                            )
                            repo.create(article)
                            imported += 1

                        session.commit()
                        st.success(f"âœ… {imported} ä»¶ã®è¨˜äº‹ã‚’å–ã‚Šè¾¼ã¿ã¾ã—ãŸï¼")
                        del st.session_state["preview_articles"]
                        st.rerun()
                else:
                    st.warning("å–ã‚Šè¾¼ã‚ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")


def render_article_list() -> None:
    """Render list of existing articles with delete option."""
    from src.database.connection import get_session
    from src.database.models import ArticleStatus
    from src.repositories.article_repository import ArticleRepository

    st.markdown("### æ—¢å­˜è¨˜äº‹ä¸€è¦§")

    try:
        with get_session() as session:
            repo = ArticleRepository(session)
            articles = list(repo.get_all())

            if not articles:
                st.info("è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
                return

            st.caption(f"å…¨ {len(articles)} ä»¶")

            # Status filter
            status_filter = st.selectbox(
                "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§çµã‚Šè¾¼ã¿",
                options=["ã™ã¹ã¦"] + [s.value for s in ArticleStatus],
                key="admin_status_filter",
            )

            if status_filter != "ã™ã¹ã¦":
                articles = [a for a in articles if a.status.value == status_filter]

            # Display articles
            for article in articles:
                with st.expander(f"**{article.week_id}** - {article.title[:40]}..."):
                    col1, col2 = st.columns([3, 1])

                    with col1:
                        st.markdown(f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** {article.status.value}")
                        if article.target_persona:
                            st.markdown(f"**ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ:** {article.target_persona}")
                        if article.seo_keywords:
                            st.markdown(f"**SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:** {article.seo_keywords}")
                        if article.hook_statement:
                            st.markdown(f"**ãƒ•ãƒƒã‚¯:** {article.hook_statement[:100]}...")
                        st.caption(f"ä½œæˆæ—¥: {article.created_at.strftime('%Y-%m-%d %H:%M')}")

                    with col2:
                        # Delete button with confirmation
                        delete_key = f"delete_{article.id}"
                        confirm_key = f"confirm_delete_{article.id}"

                        if st.session_state.get(confirm_key):
                            st.warning("æœ¬å½“ã«å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
                            col_yes, col_no = st.columns(2)
                            with col_yes:
                                if st.button("ã¯ã„", key=f"yes_{article.id}"):
                                    repo.delete(article.id)
                                    session.commit()
                                    st.session_state[confirm_key] = False
                                    st.success("å‰Šé™¤ã—ã¾ã—ãŸ")
                                    st.rerun()
                            with col_no:
                                if st.button("ã„ã„ãˆ", key=f"no_{article.id}"):
                                    st.session_state[confirm_key] = False
                                    st.rerun()
                        else:
                            if st.button("ğŸ—‘ï¸ å‰Šé™¤", key=delete_key, use_container_width=True):
                                st.session_state[confirm_key] = True
                                st.rerun()

    except Exception as e:
        st.error(f"è¨˜äº‹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")


def render_knowledge_base_tab() -> None:
    """Render knowledge base management tab."""
    st.subheader("çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ç®¡ç†")

    try:
        rag_service = RAGService()

        # Show current stats
        kb_count = rag_service.get_collection_count("knowledge_base")
        archive_count = rag_service.get_collection_count("archive_index")
        embedding_info = rag_service.get_embedding_info()

        col1, col2, col3 = st.columns([2, 2, 1])
        with col1:
            st.metric("knowledge_base", f"{kb_count} ãƒãƒ£ãƒ³ã‚¯")
        with col2:
            st.metric("archive_index", f"{archive_count} ãƒãƒ£ãƒ³ã‚¯")
        with col3:
            render_help_popover(
                "â„¹ï¸",
                [
                    "knowledge_base = 91_RefDoc ã®è³‡æ–™ã‹ã‚‰ä½œã‚‹çŸ¥è­˜ãƒ™ãƒ¼ã‚¹",
                    "archive_index = DBã®éå»è¨˜äº‹/ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰ä½œã‚‹çŸ¥è­˜ãƒ™ãƒ¼ã‚¹",
                    "ãƒãƒ£ãƒ³ã‚¯æ•°ã¯æ¤œç´¢å¯¾è±¡ã®åˆ†å‰²æ•°",
                ],
            )
        st.caption(f"Embedding: {embedding_info.get('provider')} / {embedding_info.get('model')}")
        render_help_popover(
            "â„¹ï¸ Embeddingã¨ã¯ï¼Ÿ",
            "æ–‡ç« ã‚’æ¤œç´¢ã—ã‚„ã™ã„æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚",
        )

        st.divider()

        # Seed knowledge base section
        st.subheader("91_RefDoc ã‹ã‚‰ã®æŠ•å…¥")
        render_help_popover(
            "â„¹ï¸ ã“ã“ã§ä½•ã‚’ã™ã‚‹ï¼Ÿ",
            [
                "91_RefDoc ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€knowledge_base ã«åæ˜ ã—ã¾ã™ã€‚",
                "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ›´æ–°ã¯ DB ã®è¨˜äº‹/ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ archive_index ã«åæ˜ ã—ã¾ã™ã€‚",
            ],
        )

        ref_doc_path = Path(__file__).parent.parent.parent.parent / "91_RefDoc"
        if ref_doc_path.exists():
            file_count = (
                len(list(ref_doc_path.rglob("*.md")))
                + len(list(ref_doc_path.rglob("*.txt")))
                + len(list(ref_doc_path.rglob("*.json")))
                + len(list(ref_doc_path.rglob("*.pdf")))
            )
            st.info(f"ğŸ“ {ref_doc_path} ã« {file_count} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã™")
        else:
            st.warning(f"âš ï¸ {ref_doc_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆç¢ºèªã®ã¿ï¼‰", use_container_width=True):
                with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­..."):
                    result = run_seed_script("seed_knowledge_base.py", dry_run=True)
                    st.code(result, language="text")
            render_help_popover(
                "â„¹ï¸ ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³",
                "å®Ÿéš›ã®æ›´æ–°ã¯ã›ãšã€èª­ã¿è¾¼ã‚ã‚‹ä»¶æ•°ã ã‘ç¢ºèªã—ã¾ã™ã€‚",
            )

        with col2:
            if st.button("çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°", type="primary", use_container_width=True):
                with st.spinner("çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ä¸­..."):
                    result = run_seed_script("seed_knowledge_base.py", dry_run=False, extra_args=["--prune-missing"])
                    st.code(result, language="text")
                    st.success("æ›´æ–°å®Œäº†ï¼ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")
            render_help_popover(
                "â„¹ï¸ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°",
                "91_RefDoc ã®å†…å®¹ã‚’ knowledge_base ã«åæ˜ ã—ã¾ã™ï¼ˆå‰Šé™¤ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–ï¼‰ã€‚",
            )

        with col3:
            if st.button("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’æ›´æ–°", use_container_width=True):
                with st.spinner("archive_index ã‚’æ›´æ–°ä¸­..."):
                    result = run_seed_script("seed_archive_index.py", dry_run=False, extra_args=["--prune-missing"])
                    st.code(result, language="text")
                    st.success("archive_index ã®æ›´æ–°å®Œäº†ï¼")
            render_help_popover(
                "â„¹ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ›´æ–°",
                "DBã®éå»è¨˜äº‹/ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ archive_index ã«åæ˜ ã—ã¾ã™ã€‚",
            )

        if st.button("RAGæ›´æ–°ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ + ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼‰", use_container_width=True):
            with st.spinner("RAGã‚’æ›´æ–°ä¸­..."):
                kb_result = run_seed_script("seed_knowledge_base.py", dry_run=False, extra_args=["--prune-missing"])
                ar_result = run_seed_script("seed_archive_index.py", dry_run=False, extra_args=["--prune-missing"])
                st.code(kb_result + "\n" + ar_result, language="text")
                st.success("RAGæ›´æ–°å®Œäº†ï¼ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")
        render_help_popover(
            "â„¹ï¸ RAGæ›´æ–°",
            "çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–° + ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ›´æ–°ã‚’ã¾ã¨ã‚ã¦å®Ÿè¡Œã—ã¾ã™ã€‚",
        )

        st.divider()

        # Clear collections
        st.subheader("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢")
        render_help_popover(
            "â„¹ï¸ ã‚¯ãƒªã‚¢ã¨ã¯ï¼Ÿ",
            "ChromaDBã®æ¤œç´¢ç”¨ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’å‰Šé™¤ã—ã¾ã™ã€‚å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚„DBã¯æ¶ˆãˆã¾ã›ã‚“ã€‚",
        )
        st.warning("âš ï¸ ã“ã®æ“ä½œã¯å–ã‚Šæ¶ˆã›ã¾ã›ã‚“")

        col1, col2 = st.columns(2)
        with col1:
            if st.button("knowledge_base ã‚’ã‚¯ãƒªã‚¢", use_container_width=True):
                if st.session_state.get("confirm_clear_kb"):
                    rag_service.clear_collection("knowledge_base")
                    st.success("knowledge_base ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                    st.session_state.confirm_clear_kb = False
                else:
                    st.session_state.confirm_clear_kb = True
                    st.warning("ã‚‚ã†ä¸€åº¦ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç¢ºå®š")
            render_help_popover(
                "â„¹ï¸ knowledge_base ã‚¯ãƒªã‚¢",
                "91_RefDoc ç”±æ¥ã®RAGãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã™ã€‚",
            )

        with col2:
            if st.button("archive_index ã‚’ã‚¯ãƒªã‚¢", use_container_width=True):
                if st.session_state.get("confirm_clear_archive"):
                    rag_service.clear_collection("archive_index")
                    st.success("archive_index ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                    st.session_state.confirm_clear_archive = False
                else:
                    st.session_state.confirm_clear_archive = True
                    st.warning("ã‚‚ã†ä¸€åº¦ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç¢ºå®š")
            render_help_popover(
                "â„¹ï¸ archive_index ã‚¯ãƒªã‚¢",
                "DBç”±æ¥ã®RAGãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã™ã€‚",
            )

    except Exception as e:
        st.error(f"RAGã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")


def render_search_test_tab() -> None:
    """Render search test tab for RAG quality verification."""
    st.subheader("RAGæ¤œç´¢ãƒ†ã‚¹ãƒˆ")

    query = st.text_input("æ¤œç´¢ã‚¯ã‚¨ãƒª", placeholder="ä¾‹: äºˆå®Ÿç®¡ç† KPIè¨­è¨ˆ")
    top_k = st.slider("å–å¾—ä»¶æ•°", min_value=1, max_value=20, value=5)
    generate_answer = st.checkbox("ç”ŸæˆAIã§å›ç­”ã‚’ä½œæˆ")
    provider = st.radio("ãƒ¢ãƒ‡ãƒ«", ["Anthropic", "OpenAI"], horizontal=True) if generate_answer else None

    collection = st.radio(
        "æ¤œç´¢å¯¾è±¡",
        ["knowledge_base", "archive_index"],
        horizontal=True,
    )

    if st.button("æ¤œç´¢", type="primary") and query:
        try:
            rag_service = RAGService()
            results = rag_service.search(collection, query, top_k=top_k)

            if results:
                st.success(f"{len(results)} ä»¶ã®çµæœ")

                for i, result in enumerate(results, 1):
                    with st.expander(f"#{i} - è·é›¢: {result.distance:.4f}"):
                        st.markdown(f"**ID:** `{result.id}`")
                        st.markdown(f"**ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:** `{result.metadata}`")
                        st.divider()
                        st.markdown(result.content[:500] + "..." if len(result.content) > 500 else result.content)
            else:
                st.warning("çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            if generate_answer and results:
                st.divider()
                st.markdown("### ç”Ÿæˆå›ç­”ï¼ˆRAGï¼‰")
                try:
                    answer = generate_rag_answer(query, results, provider or "Anthropic")
                    st.markdown(answer)
                except Exception as e:
                    st.error(f"ç”Ÿæˆå›ç­”ã‚¨ãƒ©ãƒ¼: {e}")

        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")


def generate_rag_answer(query: str, results: list, provider: str) -> str:
    """Generate an answer using retrieved RAG results."""
    # Build compact context
    context_parts = []
    for i, r in enumerate(results, 1):
        snippet = r.content[:800] if r.content else ""
        context_parts.append(f"[{i}] {snippet}")
    context = "\n\n".join(context_parts)

    prompt = f"""ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’ä½¿ã£ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
å‡ºå…¸ã¯å¿…ãš [ç•ªå·] ã§å¼•ç”¨ã—ã¦ãã ã•ã„ï¼ˆä¾‹: [1], [2]ï¼‰ã€‚

## è³ªå•
{query}

## æ¤œç´¢çµæœ
{context}

## å›ç­”
"""

    if provider == "OpenAI":
        from src.config import get_openai_client
        client = get_openai_client()
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯çµŒå–¶ç®¡ç†ãƒ»FP&Aã®å°‚é–€å®¶ã§ã™ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
            max_tokens=800,
        )
        return response.choices[0].message.content or ""

    from src.config import get_anthropic_client
    client = get_anthropic_client()
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=800,
        messages=[{"role": "user", "content": prompt}],
    )
    return response.content[0].text if response.content else ""


def render_system_info_tab() -> None:
    """Render system information tab."""
    st.subheader("ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")

    # Database status
    st.markdown("### PostgreSQL")
    try:
        from src.database.connection import get_session
        from src.repositories.article_repository import ArticleRepository

        with get_session() as session:
            repo = ArticleRepository(session)
            articles = repo.get_all()
            st.success(f"âœ… æ¥ç¶šOK - {len(articles)} ä»¶ã®è¨˜äº‹")
    except Exception as e:
        st.error(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

    # ChromaDB status
    st.markdown("### ChromaDB")
    try:
        rag_service = RAGService()
        st.success(f"âœ… æ¥ç¶šOK - ãƒ‘ã‚¹: {rag_service.client._persist_directory}")
    except Exception as e:
        st.error(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

    # Environment info
    st.markdown("### ç’°å¢ƒå¤‰æ•°")
    from src.config import get_settings
    try:
        settings = get_settings()
        env_status = {
            "DATABASE_URL": "âœ…" if settings.database_url else "âŒ",
            "ANTHROPIC_API_KEY": "âœ…" if settings.anthropic_api_key else "âŒ",
            "OPENAI_API_KEY": "âœ…" if settings.openai_api_key else "âŒ",
            "TAVILY_API_KEY": "âœ…" if settings.tavily_api_key else "âŒ",
            "NOTE_EMAIL": "âœ…" if settings.note_email else "âŒ",
        }
        for key, status in env_status.items():
            st.text(f"{status} {key}")
    except Exception as e:
        st.error(f"è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


def run_seed_script(script_name: str, dry_run: bool = False, extra_args: list[str] | None = None) -> str:
    """Run a seeding script under scripts/."""
    script_path = Path(__file__).parent.parent.parent.parent / "scripts" / script_name

    cmd = [sys.executable, str(script_path)]
    if dry_run:
        cmd.append("--dry-run")
    if extra_args:
        cmd.extend(extra_args)

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding="utf-8",
            env={**dict(**os.environ), "PYTHONIOENCODING": "utf-8"},
            timeout=300,
            cwd=script_path.parent.parent,
        )
        return result.stdout + result.stderr
    except subprocess.TimeoutExpired:
        return "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã¦ã„ã¾ã™"
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {e}"
